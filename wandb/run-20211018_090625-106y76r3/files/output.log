[32m[10/18 09:06:30 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): Res5ROIHeads(
    (pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=2048, out_features=2, bias=True)
      (bbox_pred): Linear(in_features=2048, out_features=4, bias=True)
    )
  )
)
  0%|                                                   | 0/559 [00:00<?, ?it/s]  6%|██▌                                      | 35/559 [00:00<00:01, 346.45it/s] 13%|█████▏                                   | 70/559 [00:00<00:01, 344.24it/s] 19%|███████▌                                | 106/559 [00:00<00:01, 348.07it/s] 25%|██████████▏                             | 142/559 [00:00<00:01, 349.49it/s] 32%|████████████▋                           | 177/559 [00:00<00:01, 348.48it/s] 38%|███████████████▏                        | 212/559 [00:00<00:01, 344.73it/s] 44%|█████████████████▋                      | 247/559 [00:00<00:00, 344.43it/s] 51%|████████████████████▎                   | 283/559 [00:00<00:00, 346.78it/s] 57%|██████████████████████▊                 | 318/559 [00:00<00:00, 343.96it/s] 63%|█████████████████████████▎              | 353/559 [00:01<00:00, 344.04it/s] 69%|███████████████████████████▊            | 388/559 [00:01<00:00, 343.30it/s] 76%|██████████████████████████████▎         | 423/559 [00:01<00:00, 343.63it/s] 82%|████████████████████████████████▊       | 459/559 [00:01<00:00, 345.56it/s] 89%|███████████████████████████████████▍    | 495/559 [00:01<00:00, 348.89it/s] 95%|█████████████████████████████████████▉  | 531/559 [00:01<00:00, 349.95it/s]100%|████████████████████████████████████████| 559/559 [00:01<00:00, 346.84it/s]
[32m[10/18 09:06:32 d2.data.build]: [0mRemoved 0 images with no usable annotations. 559 images left.
[32m[10/18 09:06:32 d2.data.dataset_mapper]: [0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]
[32m[10/18 09:06:32 d2.data.build]: [0mUsing training sampler TrainingSampler
[32m[10/18 09:06:32 d2.data.common]: [0mSerializing 559 elements to byte tensors and concatenating them all ...
[32m[10/18 09:06:32 d2.data.common]: [0mSerialized dataset takes 0.19 MiB
2021-10-18 09:06:33.483892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 2048) in the checkpoint but (2, 2048) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 2048) in the checkpoint but (4, 2048) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}[0m
[34mroi_heads.box_predictor.cls_score.{bias, weight}[0m
The checkpoint state_dict contains keys that are not used by the model:
  [35mproposal_generator.anchor_generator.cell_anchors.0[0m
[32m[10/18 09:06:37 d2.engine.train_loop]: [0mStarting training from iteration 0
[32m[10/18 09:06:45 d2.utils.events]: [0m eta: 0:15:56  iter: 19  total_loss: 1.08  loss_cls: 0.6337  loss_box_reg: 0.4406  loss_rpn_cls: 0.00828  loss_rpn_loc: 0.006149  time: 0.3841  data_time: 0.0146  lr: 4.9953e-06  max_mem: 2284M
[32m[10/18 09:06:53 d2.utils.events]: [0m eta: 0:15:58  iter: 39  total_loss: 1.026  loss_cls: 0.5496  loss_box_reg: 0.457  loss_rpn_cls: 0.008634  loss_rpn_loc: 0.005052  time: 0.3933  data_time: 0.0027  lr: 9.9902e-06  max_mem: 2284M
[32m[10/18 09:07:01 d2.utils.events]: [0m eta: 0:15:52  iter: 59  total_loss: 0.8309  loss_cls: 0.4379  loss_box_reg: 0.3914  loss_rpn_cls: 0.007142  loss_rpn_loc: 0.005457  time: 0.3935  data_time: 0.0026  lr: 1.4985e-05  max_mem: 2284M
[32m[10/18 09:07:09 d2.utils.events]: [0m eta: 0:15:43  iter: 79  total_loss: 0.7378  loss_cls: 0.3448  loss_box_reg: 0.3762  loss_rpn_cls: 0.007678  loss_rpn_loc: 0.005334  time: 0.3910  data_time: 0.0025  lr: 1.998e-05  max_mem: 2284M
[32m[10/18 09:07:16 d2.utils.events]: [0m eta: 0:15:35  iter: 99  total_loss: 0.7253  loss_cls: 0.2778  loss_box_reg: 0.4311  loss_rpn_cls: 0.01245  loss_rpn_loc: 0.005226  time: 0.3903  data_time: 0.0026  lr: 2.4975e-05  max_mem: 2284M
[32m[10/18 09:07:24 d2.utils.events]: [0m eta: 0:15:27  iter: 119  total_loss: 0.6609  loss_cls: 0.2116  loss_box_reg: 0.4382  loss_rpn_cls: 0.001911  loss_rpn_loc: 0.005662  time: 0.3897  data_time: 0.0026  lr: 2.997e-05  max_mem: 2284M
[32m[10/18 09:07:32 d2.utils.events]: [0m eta: 0:15:16  iter: 139  total_loss: 0.7021  loss_cls: 0.2041  loss_box_reg: 0.4926  loss_rpn_cls: 0.006845  loss_rpn_loc: 0.005579  time: 0.3879  data_time: 0.0026  lr: 3.4965e-05  max_mem: 2284M
[32m[10/18 09:07:39 d2.utils.events]: [0m eta: 0:15:06  iter: 159  total_loss: 0.5699  loss_cls: 0.1654  loss_box_reg: 0.4027  loss_rpn_cls: 0.01103  loss_rpn_loc: 0.004273  time: 0.3875  data_time: 0.0024  lr: 3.996e-05  max_mem: 2284M
[32m[10/18 09:07:47 d2.utils.events]: [0m eta: 0:14:58  iter: 179  total_loss: 0.6254  loss_cls: 0.16  loss_box_reg: 0.4642  loss_rpn_cls: 0.005456  loss_rpn_loc: 0.004976  time: 0.3868  data_time: 0.0027  lr: 4.4955e-05  max_mem: 2284M
[32m[10/18 09:07:55 d2.utils.events]: [0m eta: 0:14:50  iter: 199  total_loss: 0.5829  loss_cls: 0.1469  loss_box_reg: 0.4201  loss_rpn_cls: 0.006435  loss_rpn_loc: 0.002916  time: 0.3863  data_time: 0.0026  lr: 4.995e-05  max_mem: 2284M
[32m[10/18 09:08:02 d2.utils.events]: [0m eta: 0:14:42  iter: 219  total_loss: 0.5534  loss_cls: 0.1229  loss_box_reg: 0.4277  loss_rpn_cls: 0.003878  loss_rpn_loc: 0.003129  time: 0.3862  data_time: 0.0027  lr: 5.4945e-05  max_mem: 2284M
[32m[10/18 09:08:10 d2.utils.events]: [0m eta: 0:14:35  iter: 239  total_loss: 0.5904  loss_cls: 0.1162  loss_box_reg: 0.4578  loss_rpn_cls: 0.005015  loss_rpn_loc: 0.002797  time: 0.3874  data_time: 0.0049  lr: 5.994e-05  max_mem: 2284M
[32m[10/18 09:08:19 d2.utils.events]: [0m eta: 0:14:28  iter: 259  total_loss: 0.5287  loss_cls: 0.1091  loss_box_reg: 0.4094  loss_rpn_cls: 0.002878  loss_rpn_loc: 0.003122  time: 0.3894  data_time: 0.0044  lr: 6.4935e-05  max_mem: 2284M
[32m[10/18 09:08:27 d2.utils.events]: [0m eta: 0:14:22  iter: 279  total_loss: 0.5832  loss_cls: 0.1088  loss_box_reg: 0.4633  loss_rpn_cls: 0.008325  loss_rpn_loc: 0.003874  time: 0.3903  data_time: 0.0033  lr: 6.993e-05  max_mem: 2284M
[32m[10/18 09:08:35 d2.utils.events]: [0m eta: 0:14:15  iter: 299  total_loss: 0.5244  loss_cls: 0.1001  loss_box_reg: 0.414  loss_rpn_cls: 0.00406  loss_rpn_loc: 0.003237  time: 0.3908  data_time: 0.0035  lr: 7.4925e-05  max_mem: 2284M
[32m[10/18 09:08:42 d2.utils.events]: [0m eta: 0:14:08  iter: 319  total_loss: 0.4991  loss_cls: 0.08805  loss_box_reg: 0.4036  loss_rpn_cls: 0.00283  loss_rpn_loc: 0.003403  time: 0.3908  data_time: 0.0028  lr: 7.992e-05  max_mem: 2284M
[32m[10/18 09:08:50 d2.utils.events]: [0m eta: 0:14:02  iter: 339  total_loss: 0.4702  loss_cls: 0.07724  loss_box_reg: 0.3752  loss_rpn_cls: 0.004901  loss_rpn_loc: 0.003333  time: 0.3912  data_time: 0.0031  lr: 8.4915e-05  max_mem: 2284M
[32m[10/18 09:08:58 d2.utils.events]: [0m eta: 0:13:53  iter: 359  total_loss: 0.4596  loss_cls: 0.09073  loss_box_reg: 0.3727  loss_rpn_cls: 0.002749  loss_rpn_loc: 0.002562  time: 0.3909  data_time: 0.0027  lr: 8.991e-05  max_mem: 2284M
[32m[10/18 09:09:06 d2.utils.events]: [0m eta: 0:13:45  iter: 379  total_loss: 0.486  loss_cls: 0.09077  loss_box_reg: 0.3964  loss_rpn_cls: 0.002974  loss_rpn_loc: 0.00232  time: 0.3910  data_time: 0.0028  lr: 9.4905e-05  max_mem: 2284M
[32m[10/18 09:09:14 d2.utils.events]: [0m eta: 0:13:38  iter: 399  total_loss: 0.4664  loss_cls: 0.06015  loss_box_reg: 0.3921  loss_rpn_cls: 0.003564  loss_rpn_loc: 0.002775  time: 0.3912  data_time: 0.0028  lr: 9.99e-05  max_mem: 2284M
[32m[10/18 09:09:22 d2.utils.events]: [0m eta: 0:13:30  iter: 419  total_loss: 0.4618  loss_cls: 0.07685  loss_box_reg: 0.3563  loss_rpn_cls: 0.002505  loss_rpn_loc: 0.003002  time: 0.3909  data_time: 0.0031  lr: 0.0001049  max_mem: 2284M
[32m[10/18 09:09:29 d2.utils.events]: [0m eta: 0:13:22  iter: 439  total_loss: 0.4182  loss_cls: 0.06017  loss_box_reg: 0.3302  loss_rpn_cls: 0.001776  loss_rpn_loc: 0.002322  time: 0.3905  data_time: 0.0030  lr: 0.00010989  max_mem: 2284M
[32m[10/18 09:09:37 d2.utils.events]: [0m eta: 0:13:14  iter: 459  total_loss: 0.4154  loss_cls: 0.07504  loss_box_reg: 0.3099  loss_rpn_cls: 0.01067  loss_rpn_loc: 0.002622  time: 0.3902  data_time: 0.0028  lr: 0.00011489  max_mem: 2284M
[32m[10/18 09:09:45 d2.utils.events]: [0m eta: 0:13:06  iter: 479  total_loss: 0.3576  loss_cls: 0.06091  loss_box_reg: 0.2775  loss_rpn_cls: 0.003253  loss_rpn_loc: 0.002965  time: 0.3901  data_time: 0.0027  lr: 0.00011988  max_mem: 2284M
[32m[10/18 09:09:52 d2.utils.events]: [0m eta: 0:12:59  iter: 499  total_loss: 0.3494  loss_cls: 0.07167  loss_box_reg: 0.2713  loss_rpn_cls: 0.004434  loss_rpn_loc: 0.003886  time: 0.3898  data_time: 0.0027  lr: 0.00012488  max_mem: 2284M
[32m[10/18 09:10:00 d2.utils.events]: [0m eta: 0:12:51  iter: 519  total_loss: 0.3232  loss_cls: 0.05554  loss_box_reg: 0.2467  loss_rpn_cls: 0.00458  loss_rpn_loc: 0.003093  time: 0.3897  data_time: 0.0028  lr: 0.00012987  max_mem: 2284M
[32m[10/18 09:10:08 d2.utils.events]: [0m eta: 0:12:43  iter: 539  total_loss: 0.289  loss_cls: 0.06233  loss_box_reg: 0.2131  loss_rpn_cls: 0.003405  loss_rpn_loc: 0.003121  time: 0.3903  data_time: 0.0037  lr: 0.00013487  max_mem: 2284M
[32m[10/18 09:10:16 d2.utils.events]: [0m eta: 0:12:36  iter: 559  total_loss: 0.2633  loss_cls: 0.05522  loss_box_reg: 0.2164  loss_rpn_cls: 0.001595  loss_rpn_loc: 0.003165  time: 0.3906  data_time: 0.0035  lr: 0.00013986  max_mem: 2284M
[32m[10/18 09:10:24 d2.utils.events]: [0m eta: 0:12:28  iter: 579  total_loss: 0.2528  loss_cls: 0.05367  loss_box_reg: 0.1839  loss_rpn_cls: 0.001291  loss_rpn_loc: 0.002931  time: 0.3905  data_time: 0.0029  lr: 0.00014486  max_mem: 2284M
[32m[10/18 09:10:32 d2.utils.events]: [0m eta: 0:12:20  iter: 599  total_loss: 0.2875  loss_cls: 0.06346  loss_box_reg: 0.2031  loss_rpn_cls: 0.00561  loss_rpn_loc: 0.002573  time: 0.3904  data_time: 0.0032  lr: 0.00014985  max_mem: 2284M
[32m[10/18 09:10:39 d2.utils.events]: [0m eta: 0:12:12  iter: 619  total_loss: 0.2673  loss_cls: 0.05211  loss_box_reg: 0.2006  loss_rpn_cls: 0.004486  loss_rpn_loc: 0.002442  time: 0.3901  data_time: 0.0028  lr: 0.00015485  max_mem: 2284M
[32m[10/18 09:10:47 d2.utils.events]: [0m eta: 0:12:03  iter: 639  total_loss: 0.2162  loss_cls: 0.0437  loss_box_reg: 0.1503  loss_rpn_cls: 0.002228  loss_rpn_loc: 0.002711  time: 0.3895  data_time: 0.0033  lr: 0.00015984  max_mem: 2284M
[32m[10/18 09:10:54 d2.utils.events]: [0m eta: 0:11:55  iter: 659  total_loss: 0.2171  loss_cls: 0.05327  loss_box_reg: 0.1656  loss_rpn_cls: 0.003203  loss_rpn_loc: 0.002675  time: 0.3892  data_time: 0.0027  lr: 0.00016484  max_mem: 2284M
[32m[10/18 09:11:02 d2.utils.events]: [0m eta: 0:11:46  iter: 679  total_loss: 0.2375  loss_cls: 0.04478  loss_box_reg: 0.1753  loss_rpn_cls: 0.001728  loss_rpn_loc: 0.002309  time: 0.3889  data_time: 0.0028  lr: 0.00016983  max_mem: 2284M
[32m[10/18 09:11:10 d2.utils.events]: [0m eta: 0:11:38  iter: 699  total_loss: 0.2354  loss_cls: 0.05551  loss_box_reg: 0.1618  loss_rpn_cls: 0.001959  loss_rpn_loc: 0.002544  time: 0.3886  data_time: 0.0027  lr: 0.00017483  max_mem: 2284M
[32m[10/18 09:11:17 d2.utils.events]: [0m eta: 0:11:30  iter: 719  total_loss: 0.221  loss_cls: 0.04079  loss_box_reg: 0.1621  loss_rpn_cls: 0.003939  loss_rpn_loc: 0.002505  time: 0.3883  data_time: 0.0026  lr: 0.00017982  max_mem: 2284M
[32m[10/18 09:11:25 d2.utils.events]: [0m eta: 0:11:22  iter: 739  total_loss: 0.2606  loss_cls: 0.05544  loss_box_reg: 0.1851  loss_rpn_cls: 0.005101  loss_rpn_loc: 0.002873  time: 0.3882  data_time: 0.0026  lr: 0.00018482  max_mem: 2284M
[32m[10/18 09:11:32 d2.utils.events]: [0m eta: 0:11:14  iter: 759  total_loss: 0.2165  loss_cls: 0.05739  loss_box_reg: 0.1489  loss_rpn_cls: 0.003079  loss_rpn_loc: 0.002503  time: 0.3879  data_time: 0.0030  lr: 0.00018981  max_mem: 2284M
[32m[10/18 09:11:40 d2.utils.events]: [0m eta: 0:11:06  iter: 779  total_loss: 0.2088  loss_cls: 0.05342  loss_box_reg: 0.1591  loss_rpn_cls: 0.001835  loss_rpn_loc: 0.002485  time: 0.3876  data_time: 0.0028  lr: 0.00019481  max_mem: 2284M
[32m[10/18 09:11:48 d2.utils.events]: [0m eta: 0:10:58  iter: 799  total_loss: 0.2732  loss_cls: 0.04485  loss_box_reg: 0.2028  loss_rpn_cls: 0.002526  loss_rpn_loc: 0.002922  time: 0.3874  data_time: 0.0027  lr: 0.0001998  max_mem: 2284M
[32m[10/18 09:11:55 d2.utils.events]: [0m eta: 0:10:50  iter: 819  total_loss: 0.2441  loss_cls: 0.05726  loss_box_reg: 0.1893  loss_rpn_cls: 0.002108  loss_rpn_loc: 0.002688  time: 0.3873  data_time: 0.0027  lr: 0.0002048  max_mem: 2284M
[32m[10/18 09:12:03 d2.utils.events]: [0m eta: 0:10:42  iter: 839  total_loss: 0.2409  loss_cls: 0.04478  loss_box_reg: 0.174  loss_rpn_cls: 0.002071  loss_rpn_loc: 0.003347  time: 0.3872  data_time: 0.0027  lr: 0.00020979  max_mem: 2284M
[32m[10/18 09:12:10 d2.utils.events]: [0m eta: 0:10:34  iter: 859  total_loss: 0.2837  loss_cls: 0.05311  loss_box_reg: 0.2158  loss_rpn_cls: 0.002147  loss_rpn_loc: 0.002224  time: 0.3868  data_time: 0.0029  lr: 0.00021479  max_mem: 2284M
[32m[10/18 09:12:18 d2.utils.events]: [0m eta: 0:10:26  iter: 879  total_loss: 0.2571  loss_cls: 0.05812  loss_box_reg: 0.1801  loss_rpn_cls: 0.00229  loss_rpn_loc: 0.002942  time: 0.3864  data_time: 0.0027  lr: 0.00021978  max_mem: 2284M
